{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm \n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASET LOADING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-09-18 04:39:23--  https://object.pouta.csc.fi/OPUS-UNPC/v1.0/moses/en-fr.txt.zip\n",
      "Resolving object.pouta.csc.fi (object.pouta.csc.fi)... 86.50.254.18, 86.50.254.19\n",
      "Connecting to object.pouta.csc.fi (object.pouta.csc.fi)|86.50.254.18|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2657208676 (2.5G) [application/zip]\n",
      "Saving to: ‘en-fr.txt.zip.1’\n",
      "\n",
      "en-fr.txt.zip.1       1%[                    ]  30.81M  7.87MB/s    eta 6m 58s ^C\n"
     ]
    }
   ],
   "source": [
    "!wget https://object.pouta.csc.fi/OPUS-UNPC/v1.0/moses/en-fr.txt.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  en-fr.txt.zip\n",
      "  inflating: README                  \n",
      "  inflating: LICENSE                 \n",
      "  inflating: UNPC.en-fr.en           \n",
      "  inflating: UNPC.en-fr.fr           \n",
      "  inflating: UNPC.en-fr.xml          \n"
     ]
    }
   ],
   "source": [
    "!unzip en-fr.txt.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30340652"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('UNPC.en-fr.en', 'r',  encoding='utf-8') as file:\n",
    "    english = file.readlines()\n",
    "\n",
    "len(english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30340652"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('UNPC.en-fr.fr', 'r',  encoding='utf-8') as file:\n",
    "    french = file.readlines()\n",
    "\n",
    "len(french)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NATIONS\\n',\n",
       " 'E\\n',\n",
       " 'Conseil Économique\\n',\n",
       " 'Distr.\\n',\n",
       " 'GÉNÉRALE\\n',\n",
       " '2 février 1999\\n',\n",
       " 'Original : FRANÇAIS\\n',\n",
       " 'COMMISSION ÉCONOMIQUE POUR L &apos; EUROPE\\n',\n",
       " 'COMITÉ DES TRANSPORTS INTÉRIEURS\\n',\n",
       " 'Groupe de travail de la construction des véhicules\\n']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "french[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined=[]\n",
    "combined.append(english[:5000])\n",
    "combined.append(french[:5000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['UNITED NATIONS\\n',\n",
       " 'E\\n',\n",
       " 'Economic and Social Council\\n',\n",
       " 'Distr.\\n',\n",
       " 'GENERAL\\n',\n",
       " '2 February 1999\\n',\n",
       " 'ENGLISH Original: FRENCH\\n',\n",
       " 'ECONOMIC COMMISSION FOR EUROPE\\n',\n",
       " 'INLAND TRANSPORT COMMITTEE\\n',\n",
       " 'Working Party on the Construction of Vehicles\\n']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NATIONS\\n',\n",
       " 'E\\n',\n",
       " 'Conseil Économique\\n',\n",
       " 'Distr.\\n',\n",
       " 'GÉNÉRALE\\n',\n",
       " '2 février 1999\\n',\n",
       " 'Original : FRANÇAIS\\n',\n",
       " 'COMMISSION ÉCONOMIQUE POUR L &apos; EUROPE\\n',\n",
       " 'COMITÉ DES TRANSPORTS INTÉRIEURS\\n',\n",
       " 'Groupe de travail de la construction des véhicules\\n']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "french[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in /data/home1/shreeyagarg/anaconda3/lib/python3.11/site-packages (0.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sentencepiece\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train SentencePiece tokenizer (or load a pre-trained one)\n",
    "sp = spm.SentencePieceProcessor()\n",
    "# sp.load('path_to_pretrained_sentencepiece_model.model')\n",
    "\n",
    "# Tokenize the source and target sentences\n",
    "source_tokens = sp.encode(english[:5000], out_type=int)\n",
    "target_tokens = sp.encode(french[:5000], out_type=int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, latent_size, input_size ):\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.project=nn.Linear(input_size, latent_size)\n",
    "        self.l_size=latent_size\n",
    "        \n",
    "    def forward(self, tokens):\n",
    "        b, n, s=tokens.shape\n",
    "        token_embeddings=self.project.forward(tokens)\n",
    "        \n",
    "        #positional Encoding\n",
    "        positions=torch.arange(n).unsqueeze(1)\n",
    "        angs = 10000**(torch.arange(self.l_size)/self.l_size).float()\n",
    "        pos_enc=torch.zeros(b, n, self.l_size)\n",
    "        pos_enc[:,0::2]=torch.sin(positions[0::2]/angs)\n",
    "        pos_enc[:, 1::2]=torch.cos(positions[1::2]/angs)\n",
    "        token_embeddings+=pos_enc\n",
    "        \n",
    "        return token_embeddings\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedSelfAttention(nn.Module):\n",
    "    def __init__(self, latent_size, dq=200, dv=200, dk=200):\n",
    "        super(MaskedSelfAttention, self).__init__()\n",
    "        self.l_size=latent_size\n",
    "        self.dq=dq\n",
    "        self.dv=dv\n",
    "        self.dk=dk\n",
    "        self.Wq=nn.Linear(self.l_size, self.dq  )\n",
    "        self.Wv=nn.Linear(self.l_size, self.dv)\n",
    "        self.Wk=nn.Linear(self.l_size, self.dk )\n",
    "        self.softmax=nn.Softmax()\n",
    "        \n",
    "    def forward(self, embeddings):\n",
    "        b,n, e=embeddings.shape\n",
    "        Q=self.Wq.forward(embeddings).view(b,n, self.heads, self.head_dim).transpose(1,2)\n",
    "        V=self.Wv.forward(embeddings).view(b,n, self.heads, self.head_dim).transpose(1,2)\n",
    "        K=self.Wk.forward(embeddings).view(b,n, self.heads, self.head_dim).transpose(1,2)\n",
    "        mask=torch.triu(torch.ones(n,n), diagonal=1)\n",
    "        A=torch.matmul(Q, K.transpose(-2, -1)) / self.dk**0.5\n",
    "        A += (mask * -1e9)\n",
    "        Ap=nn.functional.softmax(A, dim=-1)\n",
    "        \n",
    "        att_vector=torch.matmul(Ap, V)\n",
    "        att_vector=att_vector.transpose(1,2).contiguous().view(b, n, self.l_size)\n",
    "        return att_vector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, latent_size, dq=200, dk=200, dv=200, n_heads=4):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.l_size=latent_size\n",
    "        self.dq=dq\n",
    "        self.dv=dv\n",
    "        self.dk=dk\n",
    "        \n",
    "        self.heads=n_heads\n",
    "        assert self.l_size%self.heads==0\n",
    "        self.head_dim=self.l_size//self.heads\n",
    "        self.Wq=nn.Linear(self.l_size, self.dq  )\n",
    "        self.Wv=nn.Linear(self.l_size, self.dv)\n",
    "        self.Wk=nn.Linear(self.l_size, self.dk )\n",
    "        \n",
    "    def forward(self, embeddings):\n",
    "        # Query, Key and Value\n",
    "        b,n, e=embeddings.shape\n",
    "        Q=self.Wq.forward(embeddings).view(b,n, self.heads, self.head_dim).transpose(1,2)\n",
    "        V=self.Wv.forward(embeddings).view(b,n, self.heads, self.head_dim).transpose(1,2)\n",
    "        K=self.Wk.forward(embeddings).view(b,n, self.heads, self.head_dim).transpose(1,2)\n",
    "            \n",
    "        \n",
    "        # Attention Score size=(b,n,n)\n",
    "        A=torch.matmul(Q, K.transpose(-2, -1)) / self.dk**0.5\n",
    "                \n",
    "        # Attention probability score\n",
    "        Ap=nn.functional.softmax(A, dim=-1)\n",
    "        \n",
    "        att_vector=torch.matmul(Ap, V)\n",
    "        att_vector=att_vector.transpose(1,2).contiguous().view(b, n, self.l_size)\n",
    "        return att_vector \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, latent_size, dq=200, dv=200, dk=200, n_heads=4):\n",
    "        super(CrossAttention, self).__init__()\n",
    "        self.l_size=latent_size\n",
    "        self.dq=dq\n",
    "        self.dv=dv\n",
    "        self.dk=dk\n",
    "        self.heads=n_heads\n",
    "        assert self.l_size%self.heads==0\n",
    "        self.head_dim=self.l_size//self.heads\n",
    "        self.Wq=nn.Linear(self.l_size, self.dq  )\n",
    "        self.Wv=nn.Linear(self.l_size, self.dv)\n",
    "        self.Wk=nn.Linear(self.l_size, self.dk )\n",
    "        self.softmax=nn.Softmax()\n",
    "    def forward(self, Decoder_embeddnigs, Encoder_output):\n",
    "        b,n, e=Decoder_embeddnigs.shape\n",
    "        # Query, Key and Value\n",
    "        Q=self.Wq.forward(Decoder_embeddnigs).view(b,n, self.heads, self.head_dim).transpose(1,2)\n",
    "        V=self.Wv.forward(Encoder_output).view(b,n, self.heads, self.head_dim).transpose(1,2)\n",
    "        K=self.Wk.forward(Encoder_output).view(b,n, self.heads, self.head_dim).transpose(1,2)\n",
    "        \n",
    "        # Attention Score size=(b,n,n)\n",
    "        A=torch.matmul(Q, K.transpose(-2, -1)) / self.dk**0.5\n",
    "                \n",
    "        # Attention probability score\n",
    "        Ap=nn.functional.softmax(A, dim=-1)\n",
    "        \n",
    "        att_vector=torch.matmul(Ap, V)\n",
    "        att_vector=att_vector.transpose(1,2).contiguous().view(b, n, self.l_size)\n",
    "    \n",
    "        return att_vector \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, latent_size ):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.norm=nn.LayerNorm()\n",
    "        self.text_embeddings=Embeddings(latent_size, input_size)\n",
    "        self.feedforward=nn.Sequential(nn.Linear(latent_size, latent_size), \n",
    "                                       nn.GELU(),\n",
    "                                       nn.Linear(latent_size, latent_size))\n",
    "        self.attention=SelfAttention(latent_size)\n",
    "    def forward(self, data1):\n",
    "        embeddings=self.text_embeddings.forward(data1)\n",
    "        att_vector=self.attention.forward(embeddings)\n",
    "        a1=att_vector+embeddings\n",
    "        a1=self.norm(a1)\n",
    "        ff=self.feedforward.forward(a1)\n",
    "        a2=a1+ff\n",
    "        a2=self.norm(a2)\n",
    "        return a2\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_size, latent_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.text_embeddings=Embeddings(latent_size, input_size)\n",
    "        self.norm=nn.LayerNorm()\n",
    "        self.feedforward=nn.Sequential(nn.Linear(latent_size, latent_size), \n",
    "                                       nn.GELU(),\n",
    "                                       nn.Linear(latent_size, latent_size))\n",
    "        self.linear=nn.Linear()\n",
    "        self.softmax=nn.Softmax()\n",
    "        self.cross=CrossAttention(latent_size)\n",
    "        self.masked=MaskedSelfAttention(latent_size)\n",
    "    def forward(self, data2, encoder_output):\n",
    "        embeddings=self.text_embeddings.forward(data2)\n",
    "        att_vector=self.masked.forward(embeddings)\n",
    "        a1=att_vector+embeddings\n",
    "        a1=self.norm(a1)\n",
    "        cross_attn=self.cross.forward(a1, encoder_output)\n",
    "        a2=self.norm(a1+cross_attn)\n",
    "        ff=self.feedforward.forward(a2)\n",
    "        a3=self.norm(a2+ff)\n",
    "        output=self.softmax(self.linear(a3), dim=-1)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformers(nn.Module):\n",
    "    def __init__(self,input_size, latent_size ):\n",
    "        super(Transformers, self).__init__()\n",
    "        self.encoder=Encoder(input_size, latent_size)\n",
    "        self.decoder=Decoder(input_size, latent_size)\n",
    "    def forward(self, data1, data2):\n",
    "        encoder_output=self.encoder.forward(data1)\n",
    "        decoder_output=self.decoder.forward(data2, encoder_output)\n",
    "        \n",
    "        return decoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
